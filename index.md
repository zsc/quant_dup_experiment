# LLM 量化与层复制加速实验

## 实验概述

本实验书探索通过层复制技术补偿极低比特量化带来的精度损失，目标是在 M1 Pro 上实现 32B 模型的 2-bit 权重量化，同时保持极小的 PPL 损失。核心思想是通过共享权重的集成方法，用计算换取内存带宽，提高计算访存比。

**主要实验框架**：PyTorch + llama.cpp（选择理由：对 Apple Silicon 优化最好，易于扩展，社区活跃）

## 目录

### 第一章：现代 LLM 量化技术概述
- 1.1 量化基础概念
  - 权重量化 vs 激活量化
  - 量化精度对模型性能的影响
- 1.2 主流量化方法
  - GPTQ (Generative Pre-trained Transformer Quantization)
  - AWQ (Activation-aware Weight Quantization)
  - SmoothQuant
  - GGUF/GGML
- 1.3 量化挑战与计算访存比
  - 内存带宽瓶颈
  - 极低比特量化的精度损失

### 第二章：层复制补偿理论
- 2.1 核心思想：通过冗余降低量化误差
  - 共享权重的集成方法
  - 2-bit 量化的挑战与机遇
- 2.2 计算访存比优化原理
  - 单层内存驻留的优势
  - 带宽利用率分析
  - 用计算换带宽的权衡
- 2.3 层复制策略设计
  - 基础策略：相邻层共享权重
  - 进阶策略：差异化量化参数
  - 最小化训练需求的设计原则

### 第三章：实验设计与方法论
- 3.1 实验目标与评估指标
  - 目标：32B 模型在 M1 Pro 上 2-bit 权重量化
  - 评估：困惑度 (PPL) 损失
  - 性能记录：延迟、带宽利用率
- 3.2 基线模型选择
  - Qwen 系列（密集模型）
  - DeepSeek-MoE（混合专家模型）
- 3.3 实验变量设计
  - 量化位宽：2-bit, 3-bit, 4-bit 权重
  - 激活量化：8-bit, 16-bit
  - 复制策略：单次复制、多次复制、选择性复制
- 3.4 可调整的量化参数
  - 权重量化：scaling factor, zero point
  - 激活量化：scaling factor, zero point
  - 量化组大小（group size）
  - 量化网格偏移（grid offset）
  - 每通道 vs 每组量化策略
  - 激活裁剪阈值
  - 量化噪声注入水平
  - 舍入模式：最近邻 vs 随机舍入

### 第四章：密集模型实验（Qwen）
- 4.1 Qwen 模型架构分析
- 4.2 基础层复制实验
  - 全层复制 vs 选择性复制
  - 性能与精度权衡
- 4.3 差异化量化参数实验
  - Scaling factor 优化
  - 量化组大小调整
- 4.4 结果分析与讨论

### 第五章：MoE 模型实验（DeepSeek）
- 5.1 MoE 架构的特殊考虑
  - Expert 选择机制
  - 稀疏激活的影响
- 5.2 Expert 层复制策略
  - 重要 Expert 识别（基于量化敏感度）
  - 选择性 Expert 复制
  - Router/Gate 保持全精度
- 5.3 MoE 特定优化结果

### 第六章：高级优化技术
- 6.1 自适应层选择算法
  - 基于敏感度的层选择
  - 自动化层复制决策
- 6.2 共享权重集成优化
  - 差异化量化参数搜索
  - 参数组合优化策略
- 6.3 最小化训练方案
  - 仅调整量化参数的轻量级训练
  - 冻结权重的 QAT 变体

### 第七章：性能评估与分析
- 7.1 推理速度基准测试
  - Token/秒吞吐量
  - 首 Token 延迟
- 7.2 内存使用分析
  - 模型大小压缩比
  - 运行时内存占用
- 7.3 精度保持评估
  - 各种任务的 PPL 对比
  - 下游任务性能

### 第八章：实用部署指南
- 8.1 llama.cpp 集成方案
  - GGUF 格式扩展
  - 自定义算子实现
- 8.2 量化模型导出
  - 层复制元数据保存
  - 差异化参数存储
- 8.3 硬件优化
  - Apple Silicon Metal 优化
  - 内存带宽利用策略

### 第九章：结论与展望
- 9.1 实验发现总结
- 9.2 层复制方法的适用场景
- 9.3 未来研究方向
  - 更激进的量化目标
  - 自动化层复制决策
  - 硬件协同设计

### 附录
- A. 实验代码与复现指南
- B. 详细实验数据表格
- C. 相关论文引用
