# 第一章：现代 LLM 量化技术概述

## 章节概要

大语言模型（LLM）的快速发展带来了计算和存储资源的巨大挑战。一个 70B 参数的模型在 FP16 精度下需要 140GB 内存，远超消费级硬件的承载能力。量化技术通过降低数值精度来压缩模型大小，成为了在边缘设备上部署 LLM 的关键技术。

本章将系统介绍现代 LLM 量化技术的基础概念、主流方法以及面临的挑战。我们将特别关注极低比特量化（≤4-bit）的技术难点，以及计算访存比优化在实际部署中的重要性。这些内容将为后续章节中的层复制补偿技术奠定理论基础。

## 1.1 量化基础概念

### 1.1.1 什么是量化

量化（Quantization）是将连续或高精度的数值映射到离散或低精度表示的过程。在深度学习中，量化主要指将浮点数（如 FP32、FP16）转换为整数（如 INT8、INT4）的技术。

基本的均匀量化公式为：
```
x_q = round(x / s + z)
x_dequant = s * (x_q - z)
```

其中：
- `x` 是原始浮点值
- `x_q` 是量化后的整数值
- `s` 是缩放因子（scaling factor）
- `z` 是零点（zero point）

### 1.1.2 权重量化 vs 激活量化

LLM 量化涉及两个主要部分：

**权重量化（Weight Quantization）**：
- 对模型参数进行量化，是静态的、离线的过程
- 可以通过校准数据集进行优化
- 直接影响模型存储大小
- 典型精度：INT8、INT4、INT2，甚至二值化（1-bit）

**激活量化（Activation Quantization）**：
- 对中间计算结果进行量化，是动态的、在线的过程
- 需要在推理时实时计算量化参数
- 影响计算精度和内存带宽
- 典型精度：INT8、INT16，很少低于 8-bit

### 1.1.3 量化精度对模型性能的影响

量化精度与模型性能之间存在明显的权衡关系：

| 量化精度 | 模型大小压缩比 | 典型 PPL 损失 | 实用性评估 |
|---------|--------------|-------------|-----------|
| FP16 → INT8 | 2× | < 0.1 | 生产可用 |
| FP16 → INT4 | 4× | 0.2-0.5 | 需要优化技术 |
| FP16 → INT2 | 8× | > 1.0 | 挑战性大 |
| FP16 → Binary | 16× | > 5.0 | 研究阶段 |

关键观察：
- 8-bit 量化通常能保持接近无损的性能
- 4-bit 量化需要先进的优化技术（如 GPTQ、AWQ）
- 2-bit 及以下量化面临严重的精度损失，这正是本书探索层复制技术的动机

### 1.1.4 量化粒度

量化可以在不同粒度上进行：

1. **Per-tensor 量化**：整个张量共享一组量化参数
   - 优点：参数开销小，硬件友好
   - 缺点：精度损失大

2. **Per-channel 量化**：每个输出通道独立量化
   - 优点：精度提升明显
   - 缺点：需要更多量化参数

3. **Per-group 量化**：将权重分组，每组独立量化
   - 优点：在精度和效率间取得平衡
   - 缺点：需要选择合适的组大小（典型值：128）

4. **Per-token 量化**：激活值按 token 维度量化
   - 优点：适应动态范围变化
   - 缺点：计算开销较大

## 1.2 主流量化方法

### 1.2.1 GPTQ (Generative Pre-trained Transformer Quantization)

GPTQ 是目前最流行的 LLM 量化方法之一，基于最优脑损伤（Optimal Brain Damage）理论。

**核心思想**：
- 逐层量化，使用海森矩阵（Hessian）近似来选择量化顺序
- 通过补偿机制减少量化误差的累积
- 使用小规模校准数据集（典型：128 个样本）

**算法流程**：
1. 计算层权重的海森矩阵近似
2. 按重要性顺序量化权重
3. 更新剩余权重以补偿量化误差
4. 重复直到所有权重量化完成

**优势**：
- 4-bit 量化下 PPL 损失通常 < 0.5
- 无需重新训练模型
- 量化速度快（单 GPU 小时级别）

**局限**：
- 主要针对权重量化，激活保持高精度
- 2-bit 及以下性能急剧下降

### 1.2.2 AWQ (Activation-aware Weight Quantization)

AWQ 通过分析激活模式来优化权重量化，是 GPTQ 的重要改进。

**关键洞察**：
- 保护对激活值影响大的"突出权重"（salient weights）
- 基于激活统计量自适应调整量化策略

**技术特点**：
1. **激活感知缩放**：根据激活分布调整权重缩放因子
2. **混合精度策略**：重要通道使用更高精度
3. **硬件友好设计**：优化的分组大小和量化网格

**性能表现**：
- 4-bit 量化性能优于 GPTQ
- 支持高效的 INT4 推理
- 在某些任务上接近 FP16 性能

### 1.2.3 SmoothQuant

SmoothQuant 专门解决激活量化的难题，通过平滑技术降低量化难度。

**核心创新**：
- 将激活的量化难度转移到权重
- 使用平滑因子 s：W' = W·diag(s), X' = X·diag(1/s)⁻¹
- 保持数学等价：Y = X'W' = XW

**算法步骤**：
1. 统计激活值的通道级最大值
2. 计算平滑因子 s = max(|X|)^α / max(|W|)^(1-α)
3. 离线调整权重，在线调整激活
4. 应用标准 INT8 量化

**独特优势**：
- 首个实现 INT8 激活量化的实用方法
- O(1) 额外计算开销
- 与各种量化后端兼容

### 1.2.4 GGUF/GGML

GGUF（GPT-Generated Unified Format）是 llama.cpp 生态系统的量化格式。

**设计理念**：
- CPU/GPU 推理优化
- 支持多种量化类型混合
- 文件格式自包含元数据

**量化类型**：
- Q4_0：4-bit 量化，32 个权重一组
- Q4_K_M：4-bit 量化，混合精度，针对重要层
- Q5_K_S：5-bit 量化，更好的精度权衡
- Q8_0：8-bit 量化，接近无损

**实际优势**：
- Apple Silicon 优化（Metal 加速）
- 支持模型分片和内存映射
- 活跃的开源社区支持

## 1.3 量化挑战与计算访存比

### 1.3.1 内存带宽瓶颈

现代 LLM 推理的性能瓶颈往往不是计算能力，而是内存带宽。

**瓶颈分析**：
- 计算强度：每次矩阵乘法需要加载大量权重
- 内存带宽利用率：往往只有理论峰值的 30-50%
- 批处理受限：大模型难以通过增加 batch size 来提高利用率

**具体数据**（以 M1 Pro 为例）：
- 内存带宽：200 GB/s
- 32B 模型 FP16：64 GB 权重
- 生成单个 token：需要加载全部权重一次
- 理论上限：~3 tokens/s

### 1.3.2 极低比特量化的精度损失

当量化精度降至 4-bit 以下时，面临多个技术挑战：

1. **量化误差累积**：
   - 每层的误差会逐层放大
   - 深层网络（如 80 层）的累积效应严重

2. **异常值问题**：
   - LLM 中存在少量但重要的异常值（outliers）
   - 这些值的量化误差对模型输出影响极大

3. **激活值分布**：
   - 激活值动态范围大
   - 某些层出现"巨大激活值"（massive activations）

4. **非线性失真**：
   - 极低比特量化破坏了原始的非线性关系
   - 导致模型表达能力严重下降

### 1.3.3 计算访存比优化原理

计算访存比（Compute-to-Memory Ratio）是优化推理性能的关键指标。

**定义**：
```
计算访存比 = 执行的浮点运算数 / 访问的内存字节数
```

**优化方向**：
1. **减少内存访问**：
   - 权重量化（减少每次加载的字节数）
   - 权重共享（多次使用同一权重）

2. **增加计算密度**：
   - 融合算子（减少中间结果存储）
   - 层复制（重复利用已加载的权重）

3. **本书的核心思路**：
   - 通过层复制增加计算量
   - 但权重只需加载一次
   - 用额外计算换取带宽节省

### 1.3.4 为什么需要新的补偿机制

现有量化方法在 2-bit 及以下精度时都面临困难：

1. **GPTQ/AWQ 的局限**：
   - 主要优化 4-bit 场景
   - 2-bit 时 PPL 损失 > 1.0

2. **SmoothQuant 的不足**：
   - 专注于 INT8 量化
   - 对极低比特帮助有限

3. **旋转技术的启发**：
   - QuaRot 证明了旋转可以改善量化
   - 但仍然难以突破 2-bit 瓶颈

**层复制的机遇**：
- 提供了一种全新的补偿维度
- 可以与现有技术（如旋转）结合
- 特别适合内存带宽受限的场景

## 本章小结

本章介绍了 LLM 量化的基础知识和主流技术：

1. **量化基础**：权重量化和激活量化的区别，量化粒度的选择
2. **主流方法**：GPTQ、AWQ、SmoothQuant、GGUF 各有特色
3. **核心挑战**：极低比特量化的精度损失和内存带宽瓶颈

这些挑战motivate了本书探索的层复制补偿技术。在接下来的章节中，我们将深入探讨如何通过创新的架构设计，在 2-bit 甚至 1-bit 量化下仍然保持可接受的模型性能。